from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.sql.functions import col
import pandas as pd

# Step 1: Assemble features into a single vector column
feature_columns = ['col1', 'col2', 'col3']  # Replace with your feature columns
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features_vector")
df_vector = assembler.transform(df)

# Step 2: Apply StandardScaler
scaler = StandardScaler(inputCol="features_vector", outputCol="scaled_features", withMean=True, withStd=True)
scaler_model = scaler.fit(df_vector)
scaled_df = scaler_model.transform(df_vector)

# Step 3: Extract scaled features and convert to separate columns
from pyspark.ml.functions import vector_to_array
scaled_df = scaled_df.withColumn("scaled_features_array", vector_to_array("scaled_features"))

# Split the scaled features into individual columns
for i, col_name in enumerate(feature_columns):
    scaled_df = scaled_df.withColumn(f"scaled_{col_name}", col("scaled_features_array")[i])

# Step 4: Select only the required columns (original + scaled features) and convert to Pandas
scaled_columns = [f"scaled_{col}" for col in feature_columns]
final_df = scaled_df.select(scaled_columns).toPandas()
