from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler
from pyspark.ml import Pipeline
import numpy as np
import pandas as pd
import time
from catboost import CatBoostRegressor
import matplotlib.pyplot as plt

# Create a Spark session
spark = SparkSession.builder \
    .appName("CatBoost Example") \
    .getOrCreate()

# Load your data into DataFrames (assuming train_data_feature, val_data_feature, val_data_target, val_data_key are available)

# Define columns
all_features_cols = [ ... ]  # List your feature columns here
num_f = [ ... ]              # Numerical feature columns
cat_features = [ ... ]       # Categorical feature columns

# Function to scale features
def get_feature_scaler(df, num_f):
    assembler = VectorAssembler(inputCols=num_f, outputCol='features')
    scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withMean=True, withStd=True)

    pipeline = Pipeline(stages=[assembler, scaler])
    pipeline_model = pipeline.fit(df)
    scaled_df = pipeline_model.transform(df)
    return scaled_df.select('scaled_features', *cat_features)

# Scale the training and validation data
scaler_train = get_feature_scaler(train_data_feature, num_f)
scaler_val = get_feature_scaler(val_data_feature, num_f)

# Convert to Pandas DataFrame for CatBoost
train_X = scaler_train.toPandas()
train_y = train_data_target.toPandas()
val_X = scaler_val.toPandas()

# Initialize CatBoostRegressor
catboost_train = CatBoostRegressor(n_estimators=1510,
                                    loss_function='RMSE',
                                    learning_rate=0.02,
                                    depth=8, 
                                    task_type='CPU',
                                    random_state=1,
                                    verbose=False)

# Timing the fitting process
t0 = time.time()

# Fit the model
catboost_train.fit(train_X, train_y, cat_features=cat_features)

# Make predictions
preds_val = catboost_train.predict(val_X)

# Calculate RMSE
rmse_val_s = np.sqrt(np.mean((val_y.values - preds_val) ** 2))

# Print results
print(f"Total pred lis: {sum(preds_val)} and Actual total lis (duns with nonzero lines): {val_y.sum()[0]}")
print(f'RMSE: {rmse_val_s}')

# Measure processing time
time_diff = (time.time() - t0) / 60
print(f"Data process time is {time_diff} min")

# Prepare the predictions DataFrame
cols = ['rpt_mth', 'duns_loc_num', 'zip5_cd', 'zip4_cd', 'lis_target_end_mth', 
        'jb_lis_1mth', 'naics_cd1', 'base_ga_future_3mth']
duns_pred_val = pd.concat([val_data_key[cols], pd.DataFrame(preds_val), 
                            val_data_feature[['zip54_cd']]], axis=1)
duns_pred_val.columns = ['rpt_mth', 'duns_loc_num', 'zip5_cd', 'zip4_cd', 
                         'lis_target_end_mth', 'lis_job_run', 'naics_cd', 
                         'actual_cleu_ga', 'pred_lis', 'zip54_cd']

# Function to get binned data
def get_bin_data(df, bins, pred_metrics, actual_metrics):
    df['bins'] = pd.qcut(df[pred_metrics], q=bins, duplicates='drop')
    return df.groupby('bins').agg({pred_metrics: 'mean', actual_metrics: 'mean'}).reset_index()

# Get binned data
pred_metrics = 'pred_lis'
actual_metrics = 'lis_target_end_mth'
val_duns_bins = get_bin_data(duns_pred_val, bins=20, pred_metrics=pred_metrics, actual_metrics=actual_metrics)

# Plot the results
plt.plot(val_duns_bins['bins'], val_duns_bins[pred_metrics], '-*', label=pred_metrics)
plt.plot(val_duns_bins['bins'], val_duns_bins[actual_metrics] * 1, '-o', label=actual_metrics)
plt.legend()
plt.xlabel('Bins', size=15)
plt.ylabel('Lines or ' + actual_metrics + ' (avg)', size=15)
plt.xticks(rotation=45)

# Feature importances
features = train_X.columns
importances = catboost_train.feature_importances_
indices = np.argsort(importances)

# Customized number for features
num_features = 32

plt.figure(figsize=(10, 6))
plt.title('Feature Importances')

# Only plot the customized number of features
plt.barh(range(num_features), importances[indices[-num_features:]], color='b', align='center')
plt.yticks(range(num_features), [features[i] for i in indices[-num_features:]])
plt.xlabel('Relative Importance')
plt.show()

# Create a DataFrame for feature importances
feature_imp_df = pd.concat([pd.DataFrame(train_X.columns), pd.DataFrame(catboost_train.feature_importances_)], axis=1)
feature_imp_df.columns = ['feature_names', 'importance_factor']
feature_imp_df_s = feature_imp_df.sort_values(by='importance_factor', ascending=False).reset_index(drop=True)

# Display top 20 features
print(feature_imp_df_s.head(20))

# Plot for top 10 features
num_features = 10
plt.figure(figsize=(10, 6))
plt.title('Feature Importances', size=15)

# Only plot the customized number of features
plt.barh(range(num_features), importances[indices[-num_features:]], color='b', align='center')
plt.yticks(range(num_features), [features[i] for i in indices[-num_features:]])
plt.xlabel('Relative Importance', size=15)
plt.show()
