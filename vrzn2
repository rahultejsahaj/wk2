# Set chunk size
chunk_size = 500_000

# Initialize an empty list to hold results
predictions = []

# Read and predict in chunks
for start in range(0, len(data), chunk_size):
    chunk = data[start:start + chunk_size]
    chunk_predictions = model.predict(chunk[feature_columns])
    
    # Append predictions to the list
    predictions.extend(chunk_predictions)

# Convert predictions to a DataFrame if needed
predictions_df = pd.DataFrame(predictions, columns=["prediction"])







import pandas as pd
from catboost import CatBoostRegressor
from concurrent.futures import ThreadPoolExecutor

# Load the CatBoost model
model = CatBoostRegressor()
model.load_model("path/to/your_model.cbm")

# Set batch size and number of threads based on system capabilities
batch_size = 5_000_000
num_threads = 8  # Adjust based on CPU cores

# Function to process a batch and predict
def process_batch(chunk):
    return model.predict(chunk[feature_columns])

# List to store predictions
predictions = []

# Create thread pool
with ThreadPoolExecutor(max_workers=num_threads) as executor:
    # Split data into batches and process in parallel
    for start in range(0, len(data), batch_size):
        chunk = data[start:start + batch_size]
        predictions.extend(executor.submit(process_batch, chunk).result())

# Save predictions to disk
predictions_df = pd.DataFrame(predictions, columns=["prediction"])
predictions_df.to_csv("path/to/save_predictions.csv", index=False)







# Set larger chunk size based on available memory
chunk_size = 5_000_000

# Repartition Spark DataFrame to use fewer, larger chunks
df = df.repartition(df.count() // chunk_size)

# Define the UDF as before
from pyspark.sql.functions import pandas_udf, PandasUDFType, col
import pandas as pd
from catboost import CatBoostRegressor

model = CatBoostRegressor()
model.load_model("path/to/your_model.cbm")

@pandas_udf("float", PandasUDFType.SCALAR)
def predict_catboost_udf(*cols) -> pd.Series:
    input_data = pd.concat(cols, axis=1)
    return pd.Series(model.predict(input_data))

# Apply the UDF on larger partitions
df = df.withColumn("prediction", predict_catboost_udf(*[col(c) for c in feature_columns]))

# Save the predictions with a suitable format
df.write.mode("overwrite").parquet("path/to/save_predictions")

