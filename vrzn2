from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.linalg import Vectors

# Initialize Spark session
spark = SparkSession.builder.appName("StandardScalerExample").getOrCreate()

# Sample DataFrame with numerical columns
data = [(1.0, 2.0, 10.0), (3.0, 4.0, 20.0), (5.0, 6.0, 30.0)]
df = spark.createDataFrame(data, ["feature1", "feature2", "feature3"])

# Combine all features into a single vector column using VectorAssembler
assembler = VectorAssembler(inputCols=["feature1", "feature2", "feature3"], outputCol="features")
df_vector = assembler.transform(df)

# Apply StandardScaler
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withMean=True, withStd=True)
scaler_model = scaler.fit(df_vector)

# Apply scaling
scaled_df = scaler_model.transform(df_vector)

# Show the scaled features
scaled_df.select("scaledFeatures").show(truncate=False)

# Get the mean and std used for scaling in PySpark
mean_spark = scaler_model.mean.toArray()
std_spark = scaler_model.std.toArray()

print("Mean (PySpark):", mean_spark)
print("Std Dev (PySpark):", std_spark)

# Stop Spark session
spark.stop()



import pandas as pd
from sklearn.preprocessing import StandardScaler

# Example DataFrame with numerical data
data = {
    'feature1': [1.0, 3.0, 5.0],
    'feature2': [2.0, 4.0, 6.0],
    'feature3': [10.0, 20.0, 30.0]
}
df = pd.DataFrame(data)

# Initialize StandardScaler
scaler = StandardScaler()

# Apply StandardScaler to all numerical columns
scaled_data = scaler.fit_transform(df)

# Get the mean and std used for scaling
mean_sklearn = scaler.mean_
scale_sklearn = scaler.scale_

# If you want it back as a DataFrame
scaled_df = pd.DataFrame(scaled_data, columns=df.columns)

print("Scaled DataFrame (scikit-learn):")
print(scaled_df)

# Print the mean and standard deviation
print("Mean (scikit-learn):", mean_sklearn)
print("Std Dev (scikit-learn):", scale_sklearn)
