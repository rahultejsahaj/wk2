from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
import pandas as pd
import numpy as np
import time
from catboost import CatBoostRegressor
from sklearn.metrics import mean_squared_error as mse
import matplotlib.pyplot as plt

# Create a Spark session
spark = SparkSession.builder \
    .appName("CatBoost in PySpark") \
    .getOrCreate()

# Define the feature columns
all_features_cols = ['emp_tot_num', 'naics_cd1', 'wireless_bill_tot_amt', 'mob_emp_tot_loc_num', 'mob_broadband_bill_tot_amt', 'pq_cleu_ga_1mth',
                     'pq_nl_ga_1mth', 'pq_base_ga_1mth', 'pq_phone_ga_1mth', 'pq_tablet_ga_1mth', 'pq_mbb_ga_1mth', 'pq_lis_1mth', 'pq_tablet_lis_1mth',
                     'pq_mbb_lis_1mth', 'pq_smartphone_lis_1mth', 'base_ga_previous_1mth', 'zip5_cleu_sum', 'zip5_nl_ga_sum', 'zip5_base_ga_sum',
                     'zip5_lis_sum', 'zip5_cd', 'zip4_cd', 'zip54_cleu_sum', 'zip54_nl_ga_sum', 'zip54_base_ga_sum', 'zip54_lis_sum', 'pq_cleu_ga_3mth',
                     'pq_nl_ga_3mth', 'pq_base_ga_3mth', 'pq_phone_ga_3mth', 'pq_tablet_ga_3mth', 'pq_mbb_ga_3mth', 'pq_lis_3mth', 'pq_tablet_lis_3mth',
                     'pq_mbb_lis_3mth', 'pq_smartphone_lis_3mth', 'chgd_wireless_provider_index', 'city_nm', 'base_yr_emp', 'base_yr_sls', 'bus_nm',
                     'ceo_title', 'pct_growth_emp', 'pct_growth_sls', 'population_cd', 'residence_cd', 'segmt_cd', 'sls_cd', 'sls_tot', 'subsidiary_cd',
                     'st_cnty_cd', 'territory_cd', 'trend_yr_emp', 'trend_yr_sls', 'wireless_data_apps_index', 'wireless_push_to_talk_index',
                     'wireless_voice_apps_index', 'business_age', 'ratio_cleu_subs_p_1mth', 'ratio_cleu_subs_p_3mth', 'total_employee_count', 
                     'annual_revenue', 'vz_connect', 'intent_mobility_10', 'intent_wls_bi_10', 'intent_networks_10', 'wireless_12_month_spend',
                     'wireline_12_month_spend', 'connect_12_month_spend', 'fg_customer_loc', 'fg_pro_10', 'ltebi_lq_10', 'exist_ltebi_customer_10',
                     'ltebi_wfh_10', 'wls_promo_10', 'ports_lines', 'linkedin_jobs_posted', 'linkedin_hiring_10', 'wls_wallet_share', 'wln_wallet_share',
                     'fios_loc', 'covered_prc', 'nfs_voice_perc', 'nfs_data_perc', 'future_5g_availability_10', 'zip5_duns_cnt', 'zip5_vz_cust_cnt',
                     'zip5_prospect_cnt', 'zip54_duns_cnt', 'zip54_vz_cust_cnt', 'zip54_prospect_cnt', 'wln_rel', 'con_rel', 'has_vz_rel', 
                     'wln_ser_loc', 'vz_hq_rel', 'zip5_qes_score_sum', 'zip54_qes_score_sum', 'qes_score']

num_f = [ ... ]  # Fill in as per your original code

cat_features = [ ... ]  # Fill in as per your original code

def get_feature_scaler(df, all_features_cols, num_f, cat_features):
    # Convert to Spark DataFrame
    df_scaler = df.select(all_features_cols)
    
    # VectorAssembler to combine numerical features
    assembler = VectorAssembler(inputCols=num_f, outputCol="features")
    df_vector = assembler.transform(df_scaler)
    
    # Apply StandardScaler
    scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withMean=True, withStd=True)
    scaler_model = scaler.fit(df_vector)
    scaled_df1 = scaler_model.transform(df_vector)
    
    # Convert back to Pandas DataFrame for further processing
    scaler_df2 = scaled_df1.toPandas()
    
    # Separate scaled features
    scaled_features = pd.DataFrame(scaler_df2['scaledFeatures'].tolist(), columns=num_f)
    scaler_df = scaler_df2.drop(columns=['scaledFeatures', 'features'])
    scaler_df[num_f] = scaled_features
    
    # Combine with categorical features
    df_s = df.reset_index(drop=True)
    scaler_df_final = pd.concat([scaler_df, df_s[cat_features]], axis=1)
    
    return scaler_df_final

# Assuming train_data_feature and val_data_feature are Spark DataFrames
scaler_train = get_feature_scaler(train_data_feature, all_features_cols, num_f, cat_features)
scaler_val = get_feature_scaler(val_data_feature, all_features_cols, num_f, cat_features)
full_scaler_val = get_feature_scaler(full_val_data_feature, all_features_cols, num_f, cat_features)

# Train CatBoostRegressor
catboost_model = CatBoostRegressor(n_estimators=1510, loss_function='RMSE', learning_rate=0.02, depth=8, task_type='CPU', random_state=1, verbose=False)

# Timing the training process
t0 = time.time()
catboost_model.fit(scaler_train, train_data_target)
preds_val = catboost_model.predict(scaler_val)
rmse_val_s = np.sqrt(mse(val_data_target.values, preds_val))

print(f"Total predicted lis: {sum(preds_val)} and Actual total lis (duns with nonzero lines): {val_data_target.sum()[0]}")
print(f'RMSE: {rmse_val_s}')
time_diff = (time.time() - t0) / 60
print(f"Data processing time is {time_diff} min")

# Feature importance visualization
importances = catboost_model.feature_importances_
indices = np.argsort(importances)

# Customize number of features to display
num_features = 10

plt.figure(figsize=(10, 6))
plt.title('Feature Importances', size=15)
plt.barh(range(num_features), importances[indices[-num_features:]], color='b', align='center')
plt.yticks(range(num_features), [scaler_train.columns[i] for i in indices[-num_features:]])
plt.xlabel('Relative Importance', size=15)
plt.show()
