# Import required libraries
from sklearn.preprocessing import StandardScaler
import numpy as np
from pyspark.sql import SparkSession
from pyspark.ml.feature import StandardScaler as SparkStandardScaler
from pyspark.ml.linalg import Vectors
from pyspark.sql import Row

# Step 1: StandardScaler using scikit-learn
# Create some sample data
data = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

# Compute mean and std for sklearn
scaler_sklearn = StandardScaler()
scaled_data_sklearn = scaler_sklearn.fit_transform(data)

mean_sklearn = scaler_sklearn.mean_
scale_sklearn = scaler_sklearn.scale_

print("Scikit-learn StandardScaler output:")
print(scaled_data_sklearn)
print("Scikit-learn Mean:", mean_sklearn)
print("Scikit-learn Std Dev:", scale_sklearn)

# Step 2: StandardScaler using PySpark
# Initialize Spark session
spark = SparkSession.builder.appName("StandardScalerExample").getOrCreate()

# Create a DataFrame from the same sample data
data_spark = [(Vectors.dense([1.0, 2.0]),), (Vectors.dense([3.0, 4.0]),), (Vectors.dense([5.0, 6.0]),)]
df_spark = spark.createDataFrame(data_spark, ["features"])

# Compute Spark StandardScaler statistics
scaler = SparkStandardScaler(inputCol="features", outputCol="scaledFeatures", withMean=True, withStd=True)
scaler_model = scaler.fit(df_spark)

# Manually extract the mean and standard deviation from Spark
mean_spark = scaler_model.mean.toArray()
std_spark = scaler_model.std.toArray()

# Apply Spark StandardScaler
scaled_df_spark = scaler_model.transform(df_spark)

# Convert PySpark output to a NumPy array for comparison
scaled_data_spark = np.array(scaled_df_spark.select("scaledFeatures").rdd.map(lambda x: x[0].toArray()).collect())

print("\nPySpark StandardScaler output:")
print(scaled_data_spark)
print("PySpark Mean:", mean_spark)
print("PySpark Std Dev:", std_spark)

# Step 3: Compare both outputs
# Compare the results of scikit-learn and PySpark
comparison = np.allclose(scaled_data_sklearn, scaled_data_spark, atol=1e-6)
print("\nAre the scaled outputs equal?")
print(comparison)

# Stop Spark session
spark.stop()
