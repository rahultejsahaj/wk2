from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.sql import functions as F

# Create Spark session
spark = SparkSession.builder \
    .appName("Feature Scaling and Binning") \
    .getOrCreate()

def get_feature_scaler(df, all_features_cols, num_f, cat_features):
    # Select the relevant columns for scaling
    df_scaler = df.select(all_features_cols)
    
    # VectorAssembler to combine numerical columns into a vector column
    assembler = VectorAssembler(inputCols=num_f, outputCol="features")
    df_vector = assembler.transform(df_scaler)

    # Apply StandardScaler
    scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withMean=True, withStd=True)
    scaler_model = scaler.fit(df_vector)
    scaled_df = scaler_model.transform(df_vector)

    # Extract scaled features into separate columns
    scaled_features = scaled_df.select("scaledFeatures").rdd.map(lambda x: x[0].toArray()).collect()
    scaled_columns = spark.createDataFrame(scaled_features, num_f)
    
    # Combine scaled columns with categorical features
    scaler_df_final = scaled_columns.withColumn("row_index", F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))) \
        .join(df.select(cat_features).withColumn("row_index", F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))), "row_index") \
        .drop("row_index")

    return scaler_df_final

def get_bin_data(df, bins, pred_metrics, actual_metrics):
    # Create a new column for bins based on quantiles
    df = df.withColumn("bins", F.ntile(bins).over(Window.orderBy(pred_metrics)))
    
    # Count occurrences in each bin
    duns_cnt = df.groupBy("bins").count().withColumnRenamed("count", "duns_cnt")
    
    # Calculate mean values for the metrics in each bin
    duns_bins_means = df.groupBy("bins").agg(
        F.mean(pred_metrics).alias("mean_pred"),
        F.mean(actual_metrics).alias("mean_actual")
    )
    
    # Join the counts with means
    duns_bins_means = duns_bins_means.join(duns_cnt, "bins")
    
    return duns_bins_means

# Example usage
# df = spark.read.csv("your_data.csv", header=True, inferSchema=True)  # Load your DataFrame
# all_features_cols = [...]
# num_f = [...]
# cat_features = [...]

# scaler_train = get_feature_scaler(df, all_features_cols, num_f, cat_features)
# val_bins = get_bin_data(scaler_train, bins=20, pred_metrics='pred_lis', actual_metrics='actual_metrics')
