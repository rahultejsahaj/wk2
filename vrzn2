# Set chunk size
chunk_size = 500_000

# Initialize an empty list to hold results
predictions = []

# Read and predict in chunks
for start in range(0, len(data), chunk_size):
    chunk = data[start:start + chunk_size]
    chunk_predictions = model.predict(chunk[feature_columns])
    
    # Append predictions to the list
    predictions.extend(chunk_predictions)

# Convert predictions to a DataFrame if needed
predictions_df = pd.DataFrame(predictions, columns=["prediction"])







import pandas as pd
from catboost import CatBoostRegressor
from concurrent.futures import ThreadPoolExecutor

# Load the CatBoost model
model = CatBoostRegressor()
model.load_model("path/to/your_model.cbm")

# Set batch size and number of threads based on system capabilities
batch_size = 5_000_000
num_threads = 8  # Adjust based on CPU cores

# Function to process a batch and predict
def process_batch(chunk):
    return model.predict(chunk[feature_columns])

# List to store predictions
predictions = []

# Create thread pool
with ThreadPoolExecutor(max_workers=num_threads) as executor:
    # Split data into batches and process in parallel
    for start in range(0, len(data), batch_size):
        chunk = data[start:start + batch_size]
        predictions.extend(executor.submit(process_batch, chunk).result())

# Save predictions to disk
predictions_df = pd.DataFrame(predictions, columns=["prediction"])
predictions_df.to_csv("path/to/save_predictions.csv", index=False)







import pandas as pd
from catboost import CatBoostRegressor
from concurrent.futures import ThreadPoolExecutor

# Define batch size and number of threads based on system capabilities
batch_size = 5_000_000
num_threads = 8  # Adjust based on your CPU cores

# Function to load model and process a batch for prediction
def process_batch(chunk):
    # Load a separate model instance in each thread
    model = CatBoostRegressor()
    model.load_model("path/to/your_model.cbm")
    
    # Predict on the chunk
    return model.predict(chunk[feature_columns])

# Initialize an empty list to hold predictions
predictions = []

# Create thread pool for parallel batch processing
with ThreadPoolExecutor(max_workers=num_threads) as executor:
    # Process each chunk in parallel
    futures = []
    for start in range(0, len(data), batch_size):
        chunk = data[start:start + batch_size]
        futures.append(executor.submit(process_batch, chunk))

    # Collect results from each thread
    for future in futures:
        predictions.extend(future.result())

# Save predictions to disk
predictions_df = pd.DataFrame(predictions, columns=["prediction"])
predictions_df.to_csv("path/to/save_predictions.csv", index=False)


